"""Smart ghost analysis orchestrator."""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional, Protocol

from project_control.core.content_store import ContentStore
from project_control.analysis import (
    duplicate_detector,
    legacy_detector,
    orphan_detector,
    session_detector,
)
from project_control.analysis.import_graph_detector import detect_graph_orphans
from project_control.analysis import semantic_detector


class Detector(Protocol):
    def analyze(self, snapshot: Dict[str, Any], patterns: Dict[str, Any], content_store: ContentStore) -> List[Any]:
        ...


def _run_detector(module: Any, snapshot: Dict[str, Any], patterns: Dict[str, Any], content_store: ContentStore) -> List[Any]:
    analyzer = getattr(module, "analyze", None)
    if callable(analyzer):
        return analyzer(snapshot, patterns, content_store)
    return []


def analyze_ghost(
    snapshot: Dict[str, Any],
    patterns: Dict[str, Any],
    snapshot_path: Path,
    mode: str = "pragmatic",
    deep: bool = False,
    compare_snapshot_path: Optional[Path] = None,
) -> Dict[str, List[Any]]:
    """
    Run every ghost detector and combine their findings.
    
    Args:
        snapshot: The snapshot dictionary containing file metadata.
        patterns: Configuration patterns for detectors.
        snapshot_path: Path to snapshot.json (used to create ContentStore).
        mode: Analysis mode ("pragmatic" or "strict").
        deep: Whether to run deep analysis (import graph).
    """
    # Create ContentStore for filesystem-independent content access
    content_store = ContentStore(snapshot_path)
    
    result = {
        "orphans": _run_detector(orphan_detector, snapshot, patterns, content_store),
        "legacy": _run_detector(legacy_detector, snapshot, patterns, content_store),
        "session": _run_detector(session_detector, snapshot, patterns, content_store),
        "duplicates": _run_detector(duplicate_detector, snapshot, patterns, content_store),
        "semantic_findings": _run_detector(semantic_detector, snapshot, patterns, content_store),
        "graph_orphans": [],
        "graph": {},
        "metrics": {},
    }

    result["orphans"] = sorted(result["orphans"], key=lambda p: p.lower())
    if deep:
        graph_result = detect_graph_orphans(
            snapshot, patterns, content_store, apply_ignore=(mode == "pragmatic"), compare_snapshot_path=compare_snapshot_path
        )
        result["graph_orphans"] = graph_result["orphans"]
        result["graph"] = graph_result["graph"]
        result["metrics"] = graph_result["metrics"]
        result["anomalies"] = graph_result.get("anomalies", {})
        result["entrypoints"] = graph_result.get("entrypoints", [])
        if "drift" in graph_result:
            result["drift"] = graph_result["drift"]
    return result
